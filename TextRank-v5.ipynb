{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabbadihi/LA/blob/master/TextRank-v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZaPCXY-Q365"
      },
      "source": [
        "# TextRank: Bringing Order into Texts\n",
        "In this part you will implement [TextRank: Bringing Order into Texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) paper.\n",
        "\n",
        "[Mihalcea](https://scholar.google.com/citations?user=UetM7FgAAAAJ&hl=en) and [Tarau](https://scholar.google.com/citations?user=JUMRc-oAAAAJ&hl=en) in this paper, introduced TextRank – a **graphbased ranking model for text processing**, and show how it can be successfully used for natural language applications. In particular, they proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction.\n",
        "\n",
        "The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.\n",
        "\n",
        "When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.**This paper relies on Google's PageRank**.\n",
        "\n",
        "## Defenition\n",
        "\n",
        "Formally, let $G=(V, E)$ be a directed graph with the set of vertices $V$ and set of edges $E$, where $E$ is a subset of $V \\times V$. For a given vertex $V_i$ , let $In(V_i)$ be the set of vertices that point to it (predecessors), and let $Out(V_i)$ be the set of vertices that vertex $V_i$ points to (successors). The score of a vertex $V_i$ is defined as follows (Brin and Page, 1998):\n",
        "\n",
        "$S(V_i) = (1-d) + d* \\sum_{j \\in In(V_i)} \\frac{1}{|Out(V_j)|}S(V_j)$\n",
        "\n",
        "\n",
        "where d is a damping factor and usually set to 0.85.\n",
        "\n",
        "## Graph representation\n",
        "\n",
        "TextRank builds a weighted graph representation of a document using words as nodes and **co-ocurrence** [<sup>1</sup>](#fn1) frequencies between pairs of words as edge weights. It then applies PageRank to this graph, and treats the PageRank score of each word as its significance.\n",
        "\n",
        "<img src=\"https://github.com/shahabbadihi/LA/blob/master/textrank.png?raw=1\" width=\"400\" align=\"center\">\n",
        "\n",
        "<span id=\"fn1\"> [1]: In linguistics, co-occurrence or cooccurrence is an above-chance frequency of occurrence of two terms (also known as coincidence or concurrence) from a text corpus alongside each other in a certain order. For example, when the term \"strong coffee\" appears in a document, the term \"espresso bean\" probably also tends to occur in that document.</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://github.com/shahabbadihi/LA/raw/master/PageRank.zip'\n",
        "!7z x '/content/PageRank.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RWjMMl9RVHJ",
        "outputId": "5aaa9f80-ec50-4575-a909-935bfeecf083"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-27 06:16:25--  https://github.com/shahabbadihi/LA/raw/master/PageRank.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/shahabbadihi/LA/master/PageRank.zip [following]\n",
            "--2023-01-27 06:16:25--  https://raw.githubusercontent.com/shahabbadihi/LA/master/PageRank.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 740192 (723K) [application/zip]\n",
            "Saving to: ‘PageRank.zip’\n",
            "\n",
            "PageRank.zip        100%[===================>] 722.84K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-01-27 06:16:25 (16.1 MB/s) - ‘PageRank.zip’ saved [740192/740192]\n",
            "\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 740192 bytes (723 KiB)\n",
            "\n",
            "Extracting archive: /content/PageRank.zip\n",
            "--\n",
            "Path = /content/PageRank.zip\n",
            "Type = zip\n",
            "Physical Size = 740192\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 10\n",
            "Size:       938363\n",
            "Compressed: 740192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fVMoBzkfQ37G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import collections\n",
        "\n",
        "import nltk\n",
        "import nltk.tokenize\n",
        "\n",
        "sys.path.append(\".\")\n",
        "\n",
        "import pandas\n",
        "import page_rank\n",
        "import text_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT17bRJxQ37J",
        "outputId": "9f4c78c8-9c99-4907-9eca-afe0c4873990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu0asU3ZQ37M"
      },
      "source": [
        "## Preprocessing\n",
        "**Tokenization** is a common task in **Natural Language Processing** (NLP). It’s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like [Transformers](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=what-is-tokenization-nlp).\n",
        "\n",
        "This is the process by which a large quantity of text is divided into smaller parts called **tokens**.\n",
        "\n",
        "Natural Language toolkit has very important module [**NLTK**](https://www.nltk.org/api/nltk.tokenize.html) tokenize sentences which further comprises of sub-modules word tokenize and sentence tokenize.\n",
        "\n",
        "We use the method [word_tokenize()](https://www.geeksforgeeks.org/python-nltk-nltk-tokenizer-word_tokenize/) to split a sentence into words. Please refer to below word tokenize NLTK example to understand the theory better.\n",
        "```python\n",
        "Input: \"I love Applied linear algebra! specially the projects.\"\n",
        "Output: ['I', 'love', 'Applied', 'linear', 'algebra', '!', 'specially', 'the', 'projects', '.']\n",
        "```\n",
        "After tokenizing the document we should filter irrelevant [PoS tags](https://en.wikipedia.org/wiki/Part-of-speech_tagging) and punctuation (e.g, !, ?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jILd7EZ1Q37N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5b6d86-aa6e-4b5e-cdad-36fc675e0ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NN', ',', 'PRP$', 'NN', 'VBZ', 'NNP', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "def __preprocess_document(document, relevant_pos_tags):\n",
        "    '''\n",
        "    This function accepts a string representation \n",
        "    of a document as input, and returns a tokenized\n",
        "    list of words corresponding to that document.\n",
        "    '''\n",
        "    \n",
        "    # Tokenizing the document\n",
        "    words = nltk.tokenize.word_tokenize(document)\n",
        "    \n",
        "    # PoS tagging\n",
        "    # Your code here, use nltk.pos_tag for words and make a list of second pair\n",
        "    # print(nltk.pos_tag(words))\n",
        "    pos_tags = [value for key, value in nltk.pos_tag(words)]\n",
        "    print(pos_tags)\n",
        "    \n",
        "    # Filter out words with irrelevant POS tags\n",
        "    filtered_words = []\n",
        "    for index, word in enumerate(words):\n",
        "        word = word.lower()\n",
        "        tag = pos_tags[index]\n",
        "        # TODO: append `word` to `filtered_words` if the word is not a punctuation and pos is relevant.\n",
        "        # You can use `__is_punctuation` function and `relevant_pos_tags`\n",
        "        if not text_rank.__is_punctuation(word) and tag in relevant_pos_tags:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "__preprocess_document(\"hello, my name is Shahab!\", [\"NN\", \"ADJ\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qirzgg0BQ37P"
      },
      "source": [
        "## Ranking\n",
        "In this section, first we will implement weighted PageRank and use this function to implement textRank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkzUqFKQ37Q"
      },
      "source": [
        "### PageRank using Power method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "T2KoAYWnQ37R"
      },
      "outputs": [],
      "source": [
        "def power_iteration(transition_weights, rsp=0.15, epsilon=0.00001, max_iterations=1000):\n",
        "    # Clerical work:\n",
        "    transition_weights = pandas.DataFrame(transition_weights)\n",
        "    nodes = page_rank.__extract_nodes(transition_weights)\n",
        "    transition_weights = page_rank.__make_square(transition_weights, nodes, default=0.0)\n",
        "    transition_weights = page_rank.__ensure_rows_positive(transition_weights)\n",
        "\n",
        "    # Setup:\n",
        "    state = page_rank.__start_state(nodes)\n",
        "    transition_probabilities = page_rank.__normalize_rows(transition_weights)\n",
        "    \n",
        "    # Compute transition matrix\n",
        "    # Your code here\n",
        "    transition_matrix = transition_probabilities\n",
        "    all_cols = [col for col in transition_matrix]\n",
        "    transition_matrix[all_cols] = transition_matrix[all_cols] * (1 - rsp) + 1 / len(nodes) * rsp\n",
        "    # Power iteration:\n",
        "    # TODO: implement power method\n",
        "    # Use state.copy() for copying to old_state\n",
        "    for iteration in range(max_iterations):\n",
        "        state = state.copy().dot(transition_matrix)\n",
        "        state = state / state.abs().sum()\n",
        "        \n",
        "        \n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4-Vhz5Q37S"
      },
      "source": [
        "### TextRank algorithm\n",
        "Authors used a co-occurrence relation (as we discussed), controlled by the distance between word occurrences: **two vertices are connected** if their corresponding lexical units co-occur within a **window of maximum  words**, where  can be set anywhere from 2 to 10 word.\n",
        "\n",
        "\n",
        "The vertices added to the graph can be restricted with **syntactic filters**, which select only lexical units of a certain part of speech. One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verb. Experiments showed that **best results observed for nouns (\"NN\") and adjectives (\"ADJ\") only**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cH5kMfH5Q37T"
      },
      "outputs": [],
      "source": [
        "def textrank(document, window_size=2, rsp=0.15, relevant_pos_tags=[\"NN\", \"ADJ\"]):\n",
        "    '''\n",
        "    Accepts a string representation\n",
        "    of a document and returns Pandas matrix that maps words to their related TextRank scores.\n",
        "    Keyword arguments:\n",
        "    window_size: window of maximum words, can be set between 2 to 10. (default 2)\n",
        "    rsp:\n",
        "    relevant_pos_tags: list tags that graph is restricted by (default [\"NN\", \"ADJ\"])\n",
        "    '''\n",
        "    \n",
        "    # Tokenize document:\n",
        "    words = __preprocess_document(document, relevant_pos_tags)\n",
        "    \n",
        "    \n",
        "    # Building the weighted graph:\n",
        "    # nodes: words\n",
        "    # edge weights number of times words cooccur within a window of predetermined size\n",
        "    edge_weights = collections.defaultdict(lambda: collections.Counter())\n",
        "    for index, word in enumerate(words):\n",
        "        for other_index in range(index - window_size, index + window_size + 1):\n",
        "            if other_index >= 0 and other_index < len(words) and other_index != index:\n",
        "                other_word = words[other_index]\n",
        "                edge_weights[word][other_word] += 1.0\n",
        "\n",
        "    # Apply `power_iteration` to `edge_weights` and sort the output\n",
        "    # Your code here\n",
        "    word_probabilities = power_iteration(edge_weights)\n",
        "\n",
        "    word_probabilities.sort_values(inplace=True, ascending=False)\n",
        "\n",
        "    \n",
        "    return word_probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GARSasRbQ37U"
      },
      "source": [
        "## Apply TextRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "m4qBcmjKQ37V"
      },
      "outputs": [],
      "source": [
        "def apply_text_rank(file_name, title=\"a document\"):\n",
        "    print(\"Reading \\\"%s\\\" ...\" % title)\n",
        "    # Opening:\n",
        "    \n",
        "    file_path = os.path.join(os.path.abspath(''), file_name)\n",
        "    document = open(file_path).read()\n",
        "    document = text_rank.__ascii_only(document)\n",
        "    \n",
        "    print(\"Applying TextRank to \\\"%s\\\" ...\" % title)\n",
        "    \n",
        "    # TODO: get TextRank vector\n",
        "    keyword_scores = textrank(document)\n",
        "\n",
        "    print()\n",
        "    header = \"Keyword Significance Scores for \\\"%s\\\":\" % title\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    print(keyword_scores)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTRwFySEQ37V"
      },
      "source": [
        "### Cinderalla story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8rF3ucXfQ37W",
        "outputId": "0bfced66-61e7-4860-e413-fa4e2854104e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading \"Cinderalla\" ...\n",
            "Applying TextRank to \"Cinderalla\" ...\n",
            "['RB', 'IN', 'DT', 'NN', ',', 'EX', 'VBD', 'DT', 'NN', ',', 'WP', 'IN', 'PRP$', 'NN', 'CC', 'NN', 'NN', 'VBD', ',', 'VBD', 'DT', 'NN', 'CC', 'JJS', 'NN', 'IN', 'PDT', 'DT', 'NN', '.', 'PRP', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'WP', 'VBD', 'RB', 'IN', 'JJ', 'CC', 'NN', 'IN', 'PRP$', 'NN', '.', 'DT', 'NN', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'NNP', ',', 'WP', 'VBD', 'VBN', 'IN', 'NN', 'CC', 'VBD', 'CD', 'IN', 'DT', 'JJS', 'NNS', 'DT', 'NN', 'VBD', 'RB', 'VBN', '.', 'NNP', 'POS', 'NN', 'VBD', 'RB', 'JJ', 'IN', 'PRP$', 'NN', 'CC', 'NN', 'CC', 'VBD', 'PRP$', 'VB', 'DT', 'JJS', 'CC', 'RBS', 'JJ', 'NN', 'IN', 'DT', 'NN', '.', 'NN', 'VBD', 'DT', 'NNS', ',', 'VBD', 'DT', 'NN', 'CC', 'VBD', 'DT', 'NN', 'DT', 'IN', 'PRP$', 'NNS', 'VBN', 'IN', 'JJ', 'NNS', 'VBD', 'JJ', 'VBG', 'JJ', '.', 'RB', 'PRP', 'RB', 'VBD', 'IN', 'DT', 'NNP', 'POS', 'NN', 'VBD', 'TO', 'VB', 'DT', 'NN', ',', 'VBG', 'PDT', 'DT', 'JJ', 'NNS', 'IN', 'DT', 'NN', 'TO', 'VB', '.', 'NNP', 'POS', 'NN', 'CC', 'NNS', 'VBD', 'VBN', ',', 'CC', 'MD', 'VB', 'IN', 'NN', 'CC', 'DT', 'NN', 'DT', 'NN', 'RB', '.', 'PRP', 'VBD', 'IN', 'DT', 'JJS', 'NNS', 'IN', 'DT', 'NN', 'TO', 'VB', 'IN', 'PRP', 'VBD', 'PRP$', 'JJS', '.', 'NN', 'VBD', 'TO', 'VB', 'PRP', 'VB', 'JJ', 'IN', 'DT', 'NN', 'IN', 'PRP', 'VBD', 'JJ', 'NN', 'CC', 'IN', 'WRB', 'PRP$', 'NNS', 'VBD', 'PRP', ',', 'PRP', 'RB', 'VBD', 'PRP', 'DT', 'JJS', 'NN', '.', 'IN', 'PRP', 'VBD', 'PRP', ',', 'DT', 'JJS', 'NN', 'VBD', ',', '``', 'NN', ',', 'VBP', 'PRP', 'RB', 'VBG', 'TO', 'DT', 'NN', '.', \"''\", 'NNP', 'RB', 'VBD', 'PRP$', 'NN', 'CC', 'VBD', ',', '``', 'UH', ',', 'PRP', 'VBP', 'RB', 'VBG', 'PRP', 'IN', 'PRP', 'VBP', 'NN', 'TO', 'VB', 'CC', 'MD', 'RB', 'VB', 'IN', '.', 'RB', 'PRP', 'MD', 'VB', 'NN', '.', \"''\", '``', 'NNP', 'PRP$', 'NNS', 'TO', 'VB', 'DT', 'NN', 'NN', '.', 'PRP', 'VBP', 'RB', 'JJ', '.', \"''\", 'PRP', 'VBD', '.', 'DT', 'NNS', 'VBD', 'RB', 'CC', 'VBD', ',', '``', 'PRP', 'MD', 'VB', 'NN', 'NN', 'IN', 'DT', 'NN', 'IN', 'PRP', ',', 'PRP', 'VBP', '.', \"''\", 'WRB', 'DT', 'JJ', 'NN', 'RB', 'VBD', ',', 'NNP', 'VBD', 'PRP$', 'JJ', 'CC', 'NNS', 'TO', 'DT', 'NNP', ',', 'CC', 'MD', 'RB', 'VB', 'CC', 'VB', 'IN', 'NNS', 'IN', 'PRP', 'VBD', 'PRP', 'VBP', 'DT', 'JJ', 'NN', '.', 'IN', 'PRP', 'VBD', ',', 'NNP', 'POS', 'NN', 'NN', 'VBD', '.', '``', 'NNP', ',', 'WRB', 'VBP', 'PRP', 'VBG', '.', \"''\", 'PRP', 'VBD', '.', '``', 'PRP', 'VBP', 'TO', 'VB', 'DT', 'NN', ',', 'VBZ', 'IN', 'RB', 'RB', '.', \"''\", '``', 'UH', ',', \"''\", 'VBD', 'NNP', ',', 'IN', 'NN', '.', 'DT', 'NN', 'NN', 'VBD', 'CC', 'VBD', ',', '``', 'UH', ',', 'VB', 'IN', 'DT', 'NN', 'CC', 'VB', 'PRP', 'DT', 'NN', '.', \"''\", 'NNP', 'RB', 'VBD', 'TO', 'VB', 'DT', 'JJS', 'NN', 'PRP', 'MD', 'VB', '.', 'WRB', 'PRP', 'VBD', 'PRP', ',', 'PRP', 'RB', 'VBD', 'DT', 'NN', 'IN', 'PRP$', 'NN', ',', 'RB', 'VBG', 'PRP', 'IN', 'DT', 'JJ', 'NN', ',', 'VBN', 'IN', 'NN', 'CC', 'NN', '.', 'JJ', ',', 'PRP', 'VBD', 'NNP', 'VB', 'DT', 'NN', ',', 'CC', 'WRB', 'PRP', 'VBD', 'DT', 'NN', 'JJ', 'NNS', 'RP', 'DT', 'NN', 'NN', 'VBD', 'PRP', 'DT', 'IN', 'PRP$', 'NN', ',', 'VBG', 'PRP', 'IN', 'CD', 'JJ', 'NNS', 'CC', 'DT', 'NN', '.', '``', 'NNP', 'WP', 'VBP', 'PRP', 'VB', '.', 'NNP', 'PRP', 'RB', 'VBP', 'PRP', 'VBP', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', '.', \"''\", 'VBD', 'PRP$', 'NN', '.', '``', 'UH', 'UH', '.', \"''\", 'VBD', 'NNP', ',', '``', 'CC', 'MD', 'PRP', 'VB', 'VBG', 'IN', 'DT', ',', 'IN', 'DT', 'NNS', '.', \"''\", 'PRP$', 'NN', 'RB', 'VBD', 'PRP', 'IN', 'PRP$', 'NN', 'CC', 'RB', 'NNP', 'POS', 'NN', 'VBD', 'IN', 'DT', 'NN', 'IN', 'JJ', 'CC', 'NN', ',', 'VBG', 'IN', 'NNS', '.', 'TO', 'VB', 'PRP', 'RP', ',', 'JJ', 'NN', 'VBD', 'NNP', 'DT', 'NN', 'IN', 'NN', 'NNS', ',', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.', '``', 'DT', 'NN', 'RB', 'VBZ', 'IN', 'NN', ',', 'RB', 'NN', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'RB', ',', \"''\", 'VBD', 'DT', 'NNP', '.', 'NNP', 'VBD', 'TO', 'VB', 'IN', 'NN', ',', 'VBD', 'PRP$', 'RB', 'CC', 'VB', 'RB', 'TO', 'DT', 'NN', '.', 'WRB', 'NN', 'VBD', 'PRP$', 'NN', ',', 'DT', 'NN', 'CC', 'NN', 'VBD', 'IN', 'NN', 'VBD', 'TO', 'VB', 'IN', 'PRP$', 'NN', '.', 'DT', 'NN', 'VBD', 'PRP', ',', 'PRP', 'VBD', 'DT', 'JJ', 'NN', '.', 'DT', 'NN', 'VBD', 'RB', 'TO', 'VB', 'PRP', ',', 'VBD', 'PRP', 'TO', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'RB', 'VBD', 'PRP', 'RP', 'IN', 'DT', 'NN', '.', 'NNP', 'RB', 'VBD', 'NN', 'TO', 'VB', 'PRP$', 'NNS', ',', 'WP', 'RB', 'VBD', 'RB', 'VB', 'PRP', ',', 'CC', 'VBD', 'DT', 'IN', 'DT', 'NNS', 'DT', 'NN', 'VBD', 'VBN', 'TO', 'PRP$', 'IN', 'DT', 'NN', '.', 'DT', 'NNP', 'RB', 'VBD', 'PRP', 'NN', ',', 'CC', 'NNP', 'VBD', 'VBG', 'PRP', 'RB', 'JJ', 'IN', 'PRP', 'RB', 'VBD', 'DT', 'NN', '.', 'WRB', 'DT', 'NN', 'VBD', 'NN', ',', 'NNP', 'VBD', 'VBN', 'CC', 'VBN', 'RB', ',', 'VBG', 'CD', 'IN', 'PRP$', 'NN', 'NNS', 'IN', 'IN', 'PRP$', 'NN', '.', 'DT', 'NNP', 'VBD', 'TO', 'VB', 'PRP', ',', 'CC', 'RB', 'VBD', 'TO', 'VB', 'RP', 'DT', 'NN', 'NN', 'PRP', 'VBD', 'NN', '.', 'NNP', 'VBD', 'TO', 'VB', 'NN', ',', 'CC', 'VBD', 'RB', 'IN', 'IN', 'NN', 'CC', 'IN', 'PRP$', 'NN', 'JJ', 'NNS', '.', 'PRP', 'VBD', 'VBG', 'IN', 'NN', 'WRB', 'PRP$', 'CD', 'NNS', 'VBD', 'IN', 'PRP$', 'NN', '.', '``', 'PRP', 'VBD', 'RB', 'JJ', '.', \"''\", 'VBD', 'NNP', ',', 'VBG', 'PRP$', 'NNS', 'CC', 'VBG', 'IN', 'IN', 'PRP', 'VBD', 'VBN', 'VBG', '.', '``', 'IN', 'PRP', 'VBD', 'VBN', 'RB', 'PRP', 'MD', 'VB', 'VBN', 'DT', 'RBS', 'JJ', 'NN', ',', \"''\", 'VBD', 'DT', 'JJS', 'NN', ',', '``', 'PRP', 'VBD', 'RB', 'JJ', 'TO', 'PRP', 'CC', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NNP', '.', \"''\", '``', 'PRP$', 'NN', 'VBZ', 'DT', 'NN', 'CC', 'DT', 'NNP', 'MD', 'VB', 'NN', 'TO', 'VB', 'WP', 'PRP', 'VBD', ',', \"''\", 'VBD', 'DT', 'JJS', '.', 'DT', 'JJ', 'NNS', 'RB', 'DT', 'NN', 'VBD', 'IN', 'PRP', 'MD', 'VB', 'DT', 'NN', 'WP$', 'NN', 'NN', 'IN', 'DT', 'NN', '.', 'PRP$', 'NNS', 'VBD', 'TO', 'VB', 'DT', 'NN', 'IN', 'PDT', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'NNP', ',', 'CC', 'PRP', 'VBD', 'DT', 'IN', 'NN', '.', 'VBZ', 'RB', ',', 'PRP', 'VBD', 'VBN', 'TO', 'DT', 'CD', 'NNS', 'WP', 'VBD', 'IN', 'DT', 'PRP$', 'MD', 'TO', 'VB', 'DT', 'JJR', 'NN', '.', 'NNP', ',', 'WP', 'VBD', 'DT', ',', 'RB', 'VBD', 'TO', 'VB', 'PRP', '.', 'PRP$', 'NNS', 'VBP', 'RP', 'VBG', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'NNP', 'VBD', 'IN', 'NN', 'IN', 'DT', 'NN', 'MD', 'VB', 'DT', 'NN', '.', 'WRB', 'NNP', 'POS', 'NN', 'VBD', 'RB', 'IN', 'DT', 'NN', ',', 'PRP$', 'NNS', 'VBD', 'VBN', '.', 'NNP', 'POS', 'NN', 'NN', 'VBD', 'CC', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'VBD', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', '.', 'DT', 'NNS', 'VBD', 'TO', 'PRP$', 'NNS', 'CC', 'VBD', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'PRP', 'VBD', 'PRP', 'IN', 'DT', 'NNS', '.', 'NN', 'VBD', 'PRP', 'RP', 'CC', 'VBD', 'PRP', ',', 'VBG', 'PRP', 'VB', 'PRP', 'IN', 'DT', 'PRP$', 'NN', '.', 'NN', 'VBD', 'RB', 'VBN', 'TO', 'DT', 'NNP', ',', 'VBD', 'RB', 'JJ', 'IN', 'PRP', 'VBD', 'IN', 'DT', 'NN', '.', 'DT', 'JJ', 'NNS', 'RB', 'PRP', 'VBD', 'VBN', '.', 'NNP', ',', 'WP', 'VBD', 'DT', 'JJR', 'JJ', 'IN', 'JJ', ',', 'VBD', 'PRP', 'CD', 'NNS', 'NNS', 'IN', 'DT', 'NN', ',', 'CC', 'NN', 'VBD', 'RB', 'RB', 'IN', '.']\n",
            "\n",
            "Keyword Significance Scores for \"Cinderalla\":\n",
            "---------------------------------------------\n",
            "ball          0.061122\n",
            "godmother     0.037071\n",
            "cinderella    0.036127\n",
            "slipper       0.023681\n",
            "everyone      0.022730\n",
            "                ...   \n",
            "flick         0.006599\n",
            "coachman      0.006272\n",
            "sobs          0.006258\n",
            "garden        0.006233\n",
            "palace        0.005507\n",
            "Length: 90, dtype: float64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "apply_text_rank(\"data/Cinderalla.txt\", \"Cinderalla\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahFnsFo4Q37W"
      },
      "source": [
        "### Beauty_and_the_Beast story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5H2YDq6JQ37X",
        "outputId": "d562d77e-b415-4203-91ec-92d2e59d0909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading \"Beauty and the Beast\" ...\n",
            "Applying TextRank to \"Beauty and the Beast\" ...\n",
            "['RB', 'IN', 'DT', 'NN', 'RB', 'VBD', 'DT', 'JJ', 'NN', 'CC', 'PRP$', 'NN', '.', 'PRP', 'VBD', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'VBD', 'RB', 'JJ', '.', 'RB', 'CD', 'NN', ',', 'DT', 'NN', 'VBD', 'NN', 'TO', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'JJ', 'NN', 'VBD', 'JJ', '.', 'EX', 'VBD', 'DT', 'JJ', 'NN', 'CC', 'PDT', 'DT', 'NNS', 'IN', 'DT', 'NN', 'VBD', 'VBN', '.', 'CC', 'DT', 'NN', 'VBD', 'TO', 'VB', 'DT', 'JJ', 'NN', '.', 'PRP', 'VBD', 'TO', 'DT', 'NNS', 'CC', 'VBD', 'RB', 'JJ', '.', 'VB', 'PRP', 'VBD', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', '.', 'DT', 'JJ', 'NNS', 'VBD', 'PRP', 'NN', 'CC', 'VBD', ',', '``', 'NNP', 'PRP', 'VBD', 'TO', 'VB', 'DT', 'RBS', 'JJ', 'NN', 'IN', 'DT', 'NN', '.', 'PRP', 'MD', 'VBG', 'RB', 'CC', 'VB', 'RB', 'RB', '.', 'PRP', 'MD', 'VB', 'RB', '.', 'PDT', 'DT', 'NNS', 'VBD', 'DT', 'NN', 'CC', 'VBD', 'PRP$', 'JJ', 'NNS', '.', 'WRB', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'NN', ',', 'PRP', 'VBD', ',', '``', 'WRB', 'DT', 'NN', 'VBZ', 'JJ', 'PRP', 'MD', 'VB', 'DT', 'NN', ',', 'CC', 'NN', '.', 'DT', 'NN', 'CC', 'NN', 'VBD', 'VBN', 'CC', 'VBD', 'DT', 'NN', 'TO', 'VB', 'PRP', 'CC', 'VB', 'PRP$', 'NNS', 'RB', 'CC', 'DT', 'NN', 'VBD', 'TO', 'VB', 'RB', '.', 'WRB', 'DT', 'JJ', 'NNS', 'VBD', 'DT', 'NN', 'CC', 'JJ', 'NN', ',', 'PRP', 'VBD', ',', '``', 'PRP', 'MD', 'RB', 'VB', 'WP', 'DT', 'JJ', 'NN', 'VBZ', 'VBN', '.', 'CC', 'PRP', 'RB', 'MD', 'VB', 'PRP', 'JJ', '.', 'PRP$', 'NN', 'MD', 'RB', 'VB', 'WRB', 'PRP', 'VBZ', 'DT', 'NN', '.', 'CC', 'PRP', 'MD', 'VB', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'CD', 'NNS', '.', 'RB', ',', 'DT', 'NN', 'MD', 'VB', 'RB', 'CC', 'VB', 'PRP$', 'RP', '.', 'VBG', 'DT', ',', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBD', 'VBN', '.', 'DT', 'NN', 'VBD', 'NN', 'IN', 'VBG', 'RB', 'IN', 'DT', 'NN', 'MD', 'RB', 'VB', 'DT', 'NN', '.', 'DT', 'NN', 'VBD', 'RP', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'VBD', 'NNS', 'IN', 'NN', '.', 'NN', 'VBD', 'PRP', '.', 'NNS', 'VBD', '.', 'WRB', 'DT', 'NN', 'VBD', 'JJ', 'NNS', 'JJ', ',', 'PRP', 'VBD', 'VBG', 'IN', 'DT', 'NNS', 'WRB', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'NN', '.', '``', 'WP', 'VBZ', 'DT', '.', 'NNP', 'PRP', 'VB', '.', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'VBD', ',', '``', 'IN', 'NN', ',', 'PRP$', 'JJ', 'JJ', 'NN', '.', 'CC', 'DT', 'NN', 'VBD', 'RB', 'TO', 'VB', '.', 'CC', 'DT', 'NN', 'PRP', 'VBD', 'DT', 'NN', ',', 'PRP', 'VBD', 'TO', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', '.', 'DT', 'JJ', 'NN', 'VBD', 'PRP$', 'NN', 'TO', 'DT', 'NN', 'CC', 'DT', 'NN', 'CC', 'NN', 'VBD', 'PRP$', 'IN', 'PRP$', 'NN', 'CC', 'VBD', 'PRP$', 'IN', '.', 'PRP', 'VBD', 'RB', 'JJ', 'CC', 'VBD', 'DT', 'JJ', 'NNS', '.', 'DT', 'NNS', 'VBD', 'NN', 'IN', 'PRP', 'CC', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'RB', 'IN', 'WRB', 'DT', 'NN', 'VBD', 'RP', 'IN', 'DT', 'CD', 'NNS', ',', 'PRP', 'MD', 'RB', 'VB', 'RB', 'IN', 'DT', 'NN', '.', 'NN', ',', 'VBG', 'DT', 'NNS', 'CC', 'DT', 'NNS', 'CC', 'DT', 'NNS', 'VBD', 'RB', 'JJ', 'NN', '.', 'IN', 'DT', 'CD', 'NNS', ',', 'PRP', 'DT', 'VBD', 'RB', '.', 'DT', 'CD', 'NNS', 'VBN', '.', 'EX', 'VBD', 'DT', 'NN', 'IN', 'DT', 'RB', 'IN', 'NN', '.', 'PRP', ',', 'IN', 'IN', 'PRP$', 'NNS', ',', 'VBD', 'RB', 'IN', 'DT', 'NN', 'CC', 'VBD', 'JJ', 'NNS', '.', 'IN', 'DT', 'NN', 'VBD', 'PRP$', 'NN', 'CC', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', '.', 'PRP', 'VBD', 'TO', 'DT', 'NN', 'NN', 'CC', 'VBD', 'VBN', '.', 'DT', 'NNS', ',', 'DT', 'NNS', ',', 'DT', 'NNS', 'CC', 'DT', 'NNS', 'VBD', 'DT', 'JJ', 'NN', 'CC', 'NN', '.', 'DT', 'NN', 'VBD', 'DT', 'NN', 'CC', 'VBD', 'PRP', '.', 'DT', 'NN', 'VBD', '.', 'DT', 'NN', 'RB', 'VBD', 'DT', 'VBG', 'NN', '.', 'PRP', 'VBD', 'JJ', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBD', 'PRP$', '.', 'IN', 'DT', 'NN', ',', 'DT', 'CD', 'NNS', 'VBD', 'VBN', 'IN', 'CC', 'NN', 'VBD', 'VBG', 'RP', ',', 'CD', 'IN', 'CD', '.', 'DT', 'NN', 'VBD', 'CC', 'VBD', 'PRP$', 'NNS', '.', 'PRP', 'VBD', 'DT', 'NN', 'CC', 'VBD', '.', 'PRP', 'VBD', 'PRP', '``', 'VBP', 'PRP', 'PRP$', 'NN', '.', 'PRP', 'VBD', 'JJ', 'TO', 'VB', 'PRP$', 'NN', '.', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBD', 'IN', 'NN', 'IN', 'DT', 'JJ', '.', 'DT', 'NN', 'VBD', 'TO', 'VB', 'DT', 'NN', 'IN', 'PRP', 'VBD', 'TO', 'VB', 'IN', 'NN', 'IN', 'PRP$', 'NNS', '.', 'DT', 'NN', 'CC', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'NN', '.', 'PDT', 'DT', 'NNS', 'DT', 'NN', 'WDT', 'VBD', 'DT', 'CD', 'NNS', 'JJ', ',', 'CC', 'PRP', 'VBD', 'JJ', '.', 'NN', ',', 'PRP', 'VBD', 'VBN', 'CC', 'RB', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NNS', 'NN', 'RB', ',', 'RB', 'RB', '.']\n",
            "\n",
            "Keyword Significance Scores for \"Beauty and the Beast\":\n",
            "-------------------------------------------------------\n",
            "prince         0.082345\n",
            "princess       0.074165\n",
            "king           0.062940\n",
            "queen          0.050951\n",
            "kingdom        0.042630\n",
            "baby           0.037179\n",
            "girl           0.036224\n",
            "spindle        0.035761\n",
            "fairy          0.032812\n",
            "lady           0.029869\n",
            "palace         0.026403\n",
            "everyone       0.024609\n",
            "time           0.021454\n",
            "child          0.019617\n",
            "asleep         0.019211\n",
            "bride          0.013313\n",
            "wedding        0.013141\n",
            "way            0.012198\n",
            "sleeping       0.012110\n",
            "royal          0.012093\n",
            "rest           0.011870\n",
            "floor          0.011854\n",
            "slumber        0.011849\n",
            "fairys         0.011781\n",
            "world          0.011714\n",
            "sorry          0.011651\n",
            "spell          0.011606\n",
            "bed            0.011414\n",
            "forest         0.011348\n",
            "spinning       0.011325\n",
            "need           0.011303\n",
            "course         0.011299\n",
            "turn           0.011295\n",
            "moment         0.011195\n",
            "back           0.011171\n",
            "well           0.011154\n",
            "everybody      0.011134\n",
            "kind           0.010918\n",
            "birth          0.010896\n",
            "land           0.010893\n",
            "celebration    0.010618\n",
            "snoring        0.010616\n",
            "day            0.010591\n",
            "soon           0.010576\n",
            "sleep          0.010401\n",
            "die            0.010346\n",
            "crying         0.010238\n",
            "one            0.010062\n",
            "permission     0.009972\n",
            "love           0.009943\n",
            "speak          0.009942\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "apply_text_rank(\"data/Beauty_and_the_Beast.txt\", \"Beauty and the Beast\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXI9QT-YQ37Y"
      },
      "outputs": [],
      "source": [
        "# Optional: test textRank on another documents :))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}